{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from fight_env import fight_env\n",
    "from fight_train_func import rewardFunction, normalizeData\n",
    "import random\n",
    "\n",
    "from PPO import PPO\n",
    "\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc3 = nn.Linear(128, action_size)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        logits = self.fc3(x)\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)  # 僅一個輸出，表示狀態的價值\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MyPPO(PPO):\n",
    "    def __init__(self, env, policyNetwork, valueNetwork):\n",
    "        super().__init__(policyNetwork, valueNetwork)\n",
    "        self.env = env\n",
    "\n",
    "    def show(self):\n",
    "        device = torch.device(\"cpu\")\n",
    "        self.PolicyNetwork.to(device, dtype=torch.float32)\n",
    "        self.ValueNetwork.to(device, dtype=torch.float32)\n",
    "        data = self.env.reset()\n",
    "        while (True):    \n",
    "            self.env.render()                \n",
    "            updateData = {}\n",
    "            \n",
    "            for j in range(1, 7):\n",
    "                playerID = str(j) + \"P\"\n",
    "\n",
    "                state, _, _ = data[playerID]\n",
    "                action = self.getAction(normalizeData(state))    \n",
    "                updateData[playerID] = [self.env.actionSpace[action]]\n",
    "            data = self.env.update(updateData)  \n",
    "\n",
    "\n",
    "            if (not self.env.not_done()):\n",
    "                break\n",
    "\n",
    "    def learn(self, timeStep=10000, dataNum = 4096, lr=0.003, episode=0.2, epoch=10, batchSize=256):\n",
    "        print(\"start learning\")\n",
    "        for i in range(timeStep):\n",
    "            playtime_count = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            while (len(self.ExperienceHistory['oldstate']) < dataNum):\n",
    "                data = self.env.reset()\n",
    "                die = [False, False, False, False, False, False, False]\n",
    "                agentRewards = [0, 0, 0, 0, 0, 0, 0]\n",
    "                while (True):                \n",
    "                    updateData = {}\n",
    "                    oldStates = {}\n",
    "                    actions = {}\n",
    "                    \n",
    "                    for j in range(1, 7):\n",
    "                        playerID = str(j) + \"P\"\n",
    "                        if (self.env.lives[j] == 0):\n",
    "                            die[j] = True\n",
    "                            continue\n",
    "                        state, _, _ = data[playerID]\n",
    "                        oldStates[playerID] = state\n",
    "                        action = self.getAction(normalizeData(state))\n",
    "                       \n",
    "\n",
    "                        \n",
    "                        updateData[playerID] = [self.env.actionSpace[action]]\n",
    "                        actions[playerID] = action\n",
    "                    data = self.env.update(updateData)  \n",
    "\n",
    "                    for j in range(1, 7):\n",
    "                        if (not die[j]):\n",
    "                            playerID = str(j) + \"P\"\n",
    "                            old_state = oldStates[playerID]\n",
    "                            new_state, liveLoss, scoreUp = data[playerID]\n",
    "\n",
    "                            reward = rewardFunction(old_state, self.env.actionSpace[actions[playerID]], scoreUp, liveLoss)\n",
    "                            \n",
    "                            done = int(self.env.lives[j] == 0)\n",
    "                            \n",
    "                            self.ExperienceHistory['oldstate'].append(normalizeData(state))\n",
    "                            self.ExperienceHistory['state'].append(normalizeData(new_state))\n",
    "                            self.ExperienceHistory['action'].append(action)\n",
    "                            self.ExperienceHistory['reward'].append(reward)\n",
    "                            self.ExperienceHistory['done'].append(int(done))\n",
    "\n",
    "                            agentRewards[j] += reward\n",
    "\n",
    "                    if (not self.env.not_done()):\n",
    "                        if (i % 10 == 0):\n",
    "                            print(f\"time step:{i + 1}\", end=\" \")\n",
    "                            for j in range(1, 7):\n",
    "                                \n",
    "                                print(f\"player{j} reward: {agentRewards[j]}\", end=\",\")\n",
    "                            print()\n",
    "                        playtime_count += 1\n",
    "                        break\n",
    "            self.train(epochs=epoch, lr=lr, episode=episode, batch_size=batchSize)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = fight_env(FPS=300)\n",
    "policyNetwork = PolicyNetwork(18, 7)\n",
    "valueNetwork = ValueNetwork(18)\n",
    "agent = MyPPO(env, policyNetwork=policyNetwork, valueNetwork=valueNetwork)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn(timeStep=10000, lr=0.003, dataNum=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "for i in range(10):\n",
    "    agent.show()\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
