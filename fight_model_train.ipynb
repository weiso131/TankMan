{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.9.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from fight_env import fight_env\n",
    "from fight_train_func import rewardFunction, normalizeData\n",
    "import random\n",
    "\n",
    "from PPO import PPO\n",
    "\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc3 = nn.Linear(128, action_size)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        logits = self.fc3(x)\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)  # 僅一個輸出，表示狀態的價值\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MyPPO(PPO):\n",
    "    def __init__(self, env, policyNetwork, valueNetwork):\n",
    "        super().__init__(policyNetwork, valueNetwork)\n",
    "        self.env = env\n",
    "\n",
    "    def show(self):\n",
    "        device = torch.device(\"cpu\")\n",
    "        self.PolicyNetwork.to(device, dtype=torch.float32)\n",
    "        self.ValueNetwork.to(device, dtype=torch.float32)\n",
    "        data = self.env.reset()\n",
    "        while (True):    \n",
    "            self.env.render()                \n",
    "            updateData = {}\n",
    "            \n",
    "            for j in range(1, 7):\n",
    "                playerID = str(j) + \"P\"\n",
    "\n",
    "                state, _, _ = data[playerID]\n",
    "                action = self.getAction(normalizeData(state))    \n",
    "                updateData[playerID] = [self.env.actionSpace[action]]\n",
    "            data = self.env.update(updateData)  \n",
    "\n",
    "\n",
    "            if (not self.env.not_done()):\n",
    "                break\n",
    "\n",
    "    def learn(self, timeStep=10000, dataNum = 4096, lr=0.003, episode=0.2, epoch=10, batchSize=256):\n",
    "        print(\"start learning\")\n",
    "        for i in range(timeStep):\n",
    "            playtime_count = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            while (len(self.ExperienceHistory['oldstate']) < dataNum):\n",
    "                data = self.env.reset()\n",
    "                die = [False, False, False, False, False, False, False]\n",
    "                agentRewards = [0, 0, 0, 0, 0, 0, 0]\n",
    "                while (True):                \n",
    "                    updateData = {}\n",
    "                    oldStates = {}\n",
    "                    actions = {}\n",
    "                    \n",
    "                    for j in range(1, 7):\n",
    "                        playerID = str(j) + \"P\"\n",
    "                        if (self.env.lives[j] == 0):\n",
    "                            die[j] = True\n",
    "                            continue\n",
    "                        state, _, _ = data[playerID]\n",
    "                        oldStates[playerID] = state\n",
    "                        action = self.getAction(normalizeData(state))\n",
    "                       \n",
    "\n",
    "                        \n",
    "                        updateData[playerID] = [self.env.actionSpace[action]]\n",
    "                        actions[playerID] = action\n",
    "                    data = self.env.update(updateData)  \n",
    "\n",
    "                    for j in range(1, 7):\n",
    "                        if (not die[j]):\n",
    "                            playerID = str(j) + \"P\"\n",
    "                            old_state = oldStates[playerID]\n",
    "                            new_state, liveLoss, scoreUp = data[playerID]\n",
    "\n",
    "                            reward = rewardFunction(old_state, self.env.actionSpace[actions[playerID]], scoreUp, liveLoss)\n",
    "                            \n",
    "                            done = int(self.env.lives[j] == 0)\n",
    "                            \n",
    "                            self.ExperienceHistory['oldstate'].append(normalizeData(state))\n",
    "                            self.ExperienceHistory['state'].append(normalizeData(new_state))\n",
    "                            self.ExperienceHistory['action'].append(action)\n",
    "                            self.ExperienceHistory['reward'].append(reward)\n",
    "                            self.ExperienceHistory['done'].append(int(done))\n",
    "\n",
    "                            agentRewards[j] += reward\n",
    "\n",
    "                    if (not self.env.not_done()):\n",
    "                        if (i % 10 == 0):\n",
    "                            print(f\"time step:{i + 1}\", end=\" \")\n",
    "                            for j in range(1, 7):\n",
    "                                \n",
    "                                print(f\"player{j} reward: {agentRewards[j]}\", end=\",\")\n",
    "                            print()\n",
    "                        playtime_count += 1\n",
    "                        break\n",
    "            self.train(epochs=epoch, lr=lr, episode=episode, batch_size=batchSize)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = fight_env(FPS=300)\n",
    "policyNetwork = PolicyNetwork(11, 7)\n",
    "valueNetwork = ValueNetwork(11)\n",
    "agent = MyPPO(env, policyNetwork=policyNetwork, valueNetwork=valueNetwork)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start learning\n",
      "time step:1 player1 reward: -14.0,player2 reward: -20.5,player3 reward: -6.5,player4 reward: -24.5,player5 reward: 7.5,player6 reward: 18.0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\weiso131\\Desktop\\gameAI\\weiso_tank_env\\PPO.py:148: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step:11 player1 reward: 39.5,player2 reward: 25.0,player3 reward: 41.0,player4 reward: 34.0,player5 reward: 39.5,player6 reward: 45.5,\n",
      "time step:21 player1 reward: -7.0,player2 reward: 49.0,player3 reward: 41.0,player4 reward: -21.5,player5 reward: -7.0,player6 reward: 36.0,\n",
      "time step:21 player1 reward: -10.0,player2 reward: -13.0,player3 reward: 52.5,player4 reward: -14.0,player5 reward: -46.0,player6 reward: -52.5,\n",
      "time step:31 player1 reward: 40.0,player2 reward: -25.0,player3 reward: -340.0,player4 reward: -3.5,player5 reward: 2.0,player6 reward: 28.0,\n",
      "time step:41 player1 reward: 29.0,player2 reward: 94.5,player3 reward: 1.5,player4 reward: 2.0,player5 reward: 17.5,player6 reward: 50.0,\n",
      "time step:41 player1 reward: -19.0,player2 reward: 46.0,player3 reward: 14.0,player4 reward: -24.0,player5 reward: 40.5,player6 reward: -3.0,\n",
      "time step:51 player1 reward: 108.5,player2 reward: -12.5,player3 reward: 26.0,player4 reward: 30.0,player5 reward: 24.0,player6 reward: 40.0,\n",
      "time step:61 player1 reward: -20.0,player2 reward: 68.5,player3 reward: 47.0,player4 reward: -52.5,player5 reward: 24.0,player6 reward: -15.5,\n",
      "time step:71 player1 reward: -13.0,player2 reward: -71.5,player3 reward: -83.5,player4 reward: -84.0,player5 reward: -43.0,player6 reward: 0.0,\n",
      "time step:81 player1 reward: 0.0,player2 reward: 10.0,player3 reward: 1.5,player4 reward: 9.0,player5 reward: -1.0,player6 reward: -429.0,\n",
      "time step:91 player1 reward: -80.0,player2 reward: 81.5,player3 reward: 5.5,player4 reward: 40.5,player5 reward: 0.0,player6 reward: 20.5,\n",
      "time step:101 player1 reward: 1.0,player2 reward: 0.0,player3 reward: 0.0,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:111 player1 reward: 0.0,player2 reward: 0.0,player3 reward: -63.0,player4 reward: -15.5,player5 reward: -62.5,player6 reward: -47.0,\n",
      "time step:121 player1 reward: 9.5,player2 reward: 0.0,player3 reward: 0.0,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 16.0,\n",
      "time step:131 player1 reward: 0.0,player2 reward: 0.0,player3 reward: 29.5,player4 reward: 47.5,player5 reward: -96.5,player6 reward: 0.0,\n",
      "time step:141 player1 reward: -2.5,player2 reward: -1.5,player3 reward: 0.0,player4 reward: 0.0,player5 reward: -5.5,player6 reward: -69.0,\n",
      "time step:151 player1 reward: -117.0,player2 reward: -120.5,player3 reward: -114.5,player4 reward: 0.0,player5 reward: -124.5,player6 reward: -126.5,\n",
      "time step:161 player1 reward: -0.5,player2 reward: 20.0,player3 reward: 0.0,player4 reward: -149.5,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:171 player1 reward: 0.0,player2 reward: -103.5,player3 reward: 0.0,player4 reward: 0.0,player5 reward: -30.0,player6 reward: 0.0,\n",
      "time step:181 player1 reward: 0.0,player2 reward: -50.5,player3 reward: 10.0,player4 reward: 0.5,player5 reward: 0.5,player6 reward: 0.0,\n",
      "time step:191 player1 reward: -1.5,player2 reward: -4.5,player3 reward: -1.5,player4 reward: -119.5,player5 reward: -109.5,player6 reward: -124.0,\n",
      "time step:201 player1 reward: 0.0,player2 reward: -26.5,player3 reward: 0.5,player4 reward: -0.5,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:211 player1 reward: 0.0,player2 reward: 0.0,player3 reward: 0.0,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:221 player1 reward: 0.0,player2 reward: 0.0,player3 reward: 0.0,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:231 player1 reward: 0.0,player2 reward: 0.0,player3 reward: 0.0,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:241 player1 reward: 0.0,player2 reward: 0.0,player3 reward: 0.0,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:251 player1 reward: -92.5,player2 reward: -116.0,player3 reward: -11.0,player4 reward: 23.5,player5 reward: -162.0,player6 reward: -126.0,\n",
      "time step:261 player1 reward: -21.5,player2 reward: 0.0,player3 reward: 0.0,player4 reward: 0.5,player5 reward: 0.0,player6 reward: 0.5,\n",
      "time step:271 player1 reward: 82.0,player2 reward: 239.5,player3 reward: 0.0,player4 reward: -1.5,player5 reward: 66.5,player6 reward: -1.5,\n",
      "time step:281 player1 reward: 0.0,player2 reward: 35.0,player3 reward: 8.0,player4 reward: -0.5,player5 reward: 30.0,player6 reward: 0.0,\n",
      "time step:291 player1 reward: 0.0,player2 reward: 0.0,player3 reward: 0.0,player4 reward: -0.5,player5 reward: -0.5,player6 reward: 0.0,\n",
      "time step:301 player1 reward: 0.0,player2 reward: 0.0,player3 reward: 0.0,player4 reward: -0.5,player5 reward: 0.0,player6 reward: 0.0,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeStep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataNum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 126\u001b[0m, in \u001b[0;36mMyPPO.learn\u001b[1;34m(self, timeStep, dataNum, lr, episode, epoch, batchSize)\u001b[0m\n\u001b[0;32m    124\u001b[0m             playtime_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatchSize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\Desktop\\gameAI\\weiso_tank_env\\PPO.py:115\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self, batch_size, epochs, gamma, lr, episode, lmbda)\u001b[0m\n\u001b[0;32m    112\u001b[0m totalValueLoss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m state, nextState, action, reward, V_target, advantage, old \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m    116\u001b[0m         state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    117\u001b[0m         state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.learn(timeStep=10000, lr=0.003, dataNum=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "for i in range(10):\n",
    "    agent.show()\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
