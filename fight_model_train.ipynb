{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.9.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from fight_env import fight_env\n",
    "from fight_train_func import rewardFunction, normalizeData\n",
    "import random\n",
    "from fightAlgorithm import testDataForAgent\n",
    "from PPO import PPO\n",
    "\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)  # 僅一個輸出，表示狀態的價值\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MyPPO(PPO):\n",
    "    def __init__(self, env, policyNetwork, valueNetwork):\n",
    "        super().__init__(policyNetwork, valueNetwork)\n",
    "        self.env = env\n",
    "\n",
    "    def show(self):\n",
    "        device = torch.device(\"cpu\")\n",
    "        self.PolicyNetwork.to(device, dtype=torch.float32)\n",
    "        self.ValueNetwork.to(device, dtype=torch.float32)\n",
    "        data = self.env.reset()\n",
    "        while (True):    \n",
    "            self.env.render()                \n",
    "            updateData = {}\n",
    "            \n",
    "            for j in range(1, 7):\n",
    "                playerID = str(j) + \"P\"\n",
    "\n",
    "                state, _, _ = data[playerID]\n",
    "                action = self.getAction(normalizeData(state))    \n",
    "                updateData[playerID] = [self.env.actionSpace[action]]\n",
    "            data = self.env.update(updateData)  \n",
    "\n",
    "\n",
    "            if (not self.env.not_done()):\n",
    "                break\n",
    "\n",
    "    def learn(self, timeStep=10000, dataNum = 4096, lr=0.003, episode=0.2, epoch=10, batchSize=256):\n",
    "        print(\"start learning\")\n",
    "        for i in range(timeStep):\n",
    "            playtime_count = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            while (len(self.ExperienceHistory['oldstate']) < dataNum):\n",
    "                data = self.env.reset()\n",
    "                die = [False, False, False, False, False, False, False]\n",
    "                agentRewards = [0, 0, 0, 0, 0, 0, 0]\n",
    "                while (True):                \n",
    "                    updateData = {}\n",
    "                    oldStates = {}\n",
    "                    actions = {}\n",
    "                    end_game = not self.env.not_done()\n",
    "                    for j in range(1, 7):\n",
    "                        playerID = str(j) + \"P\"\n",
    "                        if (self.env.lives[j] == 0):\n",
    "                            die[j] = True\n",
    "                            continue\n",
    "                        \n",
    "                        state, _, _ = data[playerID]\n",
    "                        oldStates[playerID] = state\n",
    "                        action = self.getAction(normalizeData(state))\n",
    "                       \n",
    "                        \n",
    "                        updateData[playerID] = [self.env.actionSpace[action]]\n",
    "                        actions[playerID] = action\n",
    "                    data = self.env.update(updateData)  \n",
    "\n",
    "                    for j in range(1, 7):\n",
    "                        if (not die[j]):\n",
    "                            playerID = str(j) + \"P\"\n",
    "                            old_state = oldStates[playerID]\n",
    "                            new_state, liveLoss, scoreUp = data[playerID]\n",
    "                            action = actions[playerID]\n",
    "                            reward = rewardFunction(old_state, self.env.actionSpace[action], scoreUp, liveLoss)\n",
    "                            \n",
    "                            done = int(self.env.lives[j] == 0 or end_game)\n",
    "                            \n",
    "                            self.ExperienceHistory['oldstate'].append(normalizeData(old_state))\n",
    "                            self.ExperienceHistory['state'].append(normalizeData(new_state))\n",
    "                            self.ExperienceHistory['action'].append(action)\n",
    "                            self.ExperienceHistory['reward'].append(reward)\n",
    "                            self.ExperienceHistory['done'].append(done)\n",
    "\n",
    "                            agentRewards[j] += reward\n",
    "\n",
    "                                \n",
    "\n",
    "                    if (end_game):\n",
    "                        if (i % 10 == 0):\n",
    "                            print(f\"time step:{i + 1}\", end=\" \")\n",
    "                            for j in range(1, 7):\n",
    "                                \n",
    "                                print(f\"player{j} reward: {agentRewards[j]}\", end=\",\")\n",
    "                            print()\n",
    "                        playtime_count += 1\n",
    "                        break\n",
    "            self.train(epochs=epoch, lr=lr, episode=episode, batch_size=batchSize)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = fight_env(FPS=300)\n",
    "policyNetwork = PolicyNetwork(11, 7)\n",
    "valueNetwork = ValueNetwork(11)\n",
    "agent = MyPPO(env, policyNetwork=policyNetwork, valueNetwork=valueNetwork)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start learning\n",
      "time step:1 player1 reward: 6.599999999999973,player2 reward: -8.58999999999999,player3 reward: -6.5899999999999945,player4 reward: 15.479999999999936,player5 reward: -2.2799999999999954,player6 reward: -5.019999999999997,\n",
      "time step:1 player1 reward: 4.589999999999904,player2 reward: -1.4899999999999967,player3 reward: -8.219999999999997,player4 reward: 20.60000000000034,player5 reward: -7.169999999999984,player6 reward: -4.759999999999943,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\weiso131\\Desktop\\gameAI\\weiso_tank_env\\PPO.py:146: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step:11 player1 reward: 2.6899999999999524,player2 reward: -6.539999999999998,player3 reward: -4.309999999999949,player4 reward: 9.02999999999995,player5 reward: -5.6699999999999235,player6 reward: -8.269999999999872,\n",
      "time step:11 player1 reward: 14.279999999999953,player2 reward: -7.1999999999999895,player3 reward: -11.429999999999959,player4 reward: 17.01999999999991,player5 reward: -8.289999999999967,player6 reward: -6.169999999999998,\n"
     ]
    }
   ],
   "source": [
    "agent.learn(timeStep=10000, lr=0.003, dataNum=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "for i in range(10):\n",
    "    agent.show()\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
