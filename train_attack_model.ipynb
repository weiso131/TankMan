{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.9.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from train_attack_model_env import attack_env\n",
    "from attack_train_func import rewardFunction, normalizeData\n",
    "import random\n",
    "\n",
    "from PPO import PPO\n",
    "\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc3 = nn.Linear(128, action_size)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        logits = self.fc3(x)\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)  # 僅一個輸出，表示狀態的價值\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MyPPO(PPO):\n",
    "    def __init__(self, env, policyNetwork, valueNetwork):\n",
    "        super().__init__(policyNetwork, valueNetwork)\n",
    "        self.env = env\n",
    "\n",
    "    def show(self):\n",
    "        device = torch.device(\"cpu\")\n",
    "        self.PolicyNetwork.to(device, dtype=torch.float32)\n",
    "        self.ValueNetwork.to(device, dtype=torch.float32)\n",
    "        data = self.env.reset()\n",
    "        while (True):    \n",
    "            self.env.render()                \n",
    "            updateData = {}\n",
    "            \n",
    "            for j in range(1, 7):\n",
    "                playerID = str(j) + \"P\"\n",
    "\n",
    "                state, _, _ = data[playerID]\n",
    "                action = self.getAction(normalizeData(state))    \n",
    "                updateData[playerID] = [self.env.actionSpace[action]]\n",
    "            data = self.env.update(updateData)  \n",
    "\n",
    "\n",
    "            if (not self.env.not_done()):\n",
    "                break\n",
    "\n",
    "    def learn(self, timeStep=10000, dataNum = 4096, lr=0.003, episode=0.2, epoch=10, batchSize=256):\n",
    "        print(\"start learning\")\n",
    "        for i in range(timeStep):\n",
    "            playtime_count = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            while (len(self.ExperienceHistory['oldstate']) < dataNum):\n",
    "                data = self.env.reset()\n",
    "                die = [False, False, False, False, False, False, False]\n",
    "                agentRewards = [0, 0, 0, 0, 0, 0, 0]\n",
    "                while (True):                \n",
    "                    updateData = {}\n",
    "                    oldStates = {}\n",
    "                    actions = {}\n",
    "                    \n",
    "                    for j in range(1, 7):\n",
    "                        playerID = str(j) + \"P\"\n",
    "                        if (self.env.lives[j] == 0):\n",
    "                            die[j] = True\n",
    "                            continue\n",
    "                        state, _, _ = data[playerID]\n",
    "                        oldStates[playerID] = state\n",
    "                        action = self.getAction(normalizeData(state))\n",
    "                       \n",
    "\n",
    "                        \n",
    "                        updateData[playerID] = [self.env.actionSpace[action]]\n",
    "                        actions[playerID] = action\n",
    "                    data = self.env.update(updateData)  \n",
    "\n",
    "                    for j in range(1, 7):\n",
    "                        if (not die[j]):\n",
    "                            playerID = str(j) + \"P\"\n",
    "                            old_state = oldStates[playerID]\n",
    "                            new_state, liveLoss, scoreUp = data[playerID]\n",
    "\n",
    "                            reward = rewardFunction(old_state, self.env.actionSpace[actions[playerID]], scoreUp, liveLoss)\n",
    "                            \n",
    "                            done = int(self.env.lives[j] == 0)\n",
    "                            \n",
    "                            self.ExperienceHistory['oldstate'].append(normalizeData(state))\n",
    "                            self.ExperienceHistory['state'].append(normalizeData(new_state))\n",
    "                            self.ExperienceHistory['action'].append(action)\n",
    "                            self.ExperienceHistory['reward'].append(reward)\n",
    "                            self.ExperienceHistory['done'].append(int(done))\n",
    "\n",
    "                            agentRewards[j] += reward\n",
    "\n",
    "                    if (not self.env.not_done()):\n",
    "                        if (i % 10 == 0):\n",
    "                            print(f\"time step:{i + 1}\", end=\" \")\n",
    "                            for j in range(1, 7):\n",
    "                                \n",
    "                                print(f\"player{j} reward: {agentRewards[j]}\", end=\",\")\n",
    "                            print()\n",
    "                        playtime_count += 1\n",
    "                        break\n",
    "            self.train(epochs=epoch, lr=lr, episode=episode, batch_size=batchSize)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = attack_env(FPS=300)\n",
    "policyNetwork = PolicyNetwork(18, 7)\n",
    "valueNetwork = ValueNetwork(18)\n",
    "agent = MyPPO(env, policyNetwork=policyNetwork, valueNetwork=valueNetwork)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start learning\n",
      "time step:11 player1 reward: 0.0,player2 reward: 0.1340000000000001,player3 reward: 0.0,player4 reward: 20.261000000000006,player5 reward: -0.9099999999999999,player6 reward: 30.03,\n",
      "time step:21 player1 reward: 164.06100000000023,player2 reward: 0.5000000000000003,player3 reward: 85.63599999999974,player4 reward: 44.779999999999895,player5 reward: 150.91100000000034,player6 reward: 0.3060000000000002,\n",
      "time step:31 player1 reward: 0.4840000000000004,player2 reward: 0.0,player3 reward: 0.004,player4 reward: 0.03200000000000002,player5 reward: 0.7420000000000002,player6 reward: 0.014000000000000005,\n",
      "time step:41 player1 reward: 0.0,player2 reward: 0.0,player3 reward: 0.5000000000000003,player4 reward: 0.5200000000000004,player5 reward: 0.060000000000000005,player6 reward: 0.4440000000000001,\n",
      "time step:51 player1 reward: 187.0,player2 reward: 187.5,player3 reward: 207.5,player4 reward: 144.20200000000003,player5 reward: -108.685,player6 reward: 244.1,\n",
      "time step:61 player1 reward: 185.505,player2 reward: 0.5000000000000003,player3 reward: 0.5000000000000003,player4 reward: 0.0,player5 reward: 1.53,player6 reward: 0.0,\n",
      "time step:71 player1 reward: 187.5,player2 reward: 188.0,player3 reward: 0.5000000000000003,player4 reward: 185.0,player5 reward: 186.5,player6 reward: 0.0,\n",
      "time step:81 player1 reward: 0.5000000000000003,player2 reward: 0.5000000000000003,player3 reward: 0.5000000000000003,player4 reward: 0.02,player5 reward: 0.0,player6 reward: 0.051000000000000004,\n",
      "time step:91 player1 reward: 0.5000000000000003,player2 reward: 0.5000000000000003,player3 reward: 0.5000000000000003,player4 reward: -0.09399999999999988,player5 reward: 10.053999999999998,player6 reward: 10.006999999999996,\n",
      "time step:101 player1 reward: 0.0,player2 reward: 249.5,player3 reward: 53.5,player4 reward: 12.1,player5 reward: 249.60000000000002,player6 reward: 218.5,\n",
      "time step:111 player1 reward: 0.0,player2 reward: 187.5,player3 reward: 187.5,player4 reward: 0.15,player5 reward: 0.15,player6 reward: 0.2,\n",
      "time step:121 player1 reward: 0.25000000000000017,player2 reward: 0.0,player3 reward: 0.5000000000000003,player4 reward: 0.40600000000000025,player5 reward: 30.323999999999998,player6 reward: -0.45,\n",
      "time step:131 player1 reward: 187.5,player2 reward: 0.5000000000000003,player3 reward: 0.0,player4 reward: 0.13499999999999998,player5 reward: 0.128,player6 reward: 0.15,\n",
      "time step:141 player1 reward: 0.5000000000000003,player2 reward: 187.5,player3 reward: 0.5000000000000003,player4 reward: 121.13,player5 reward: 0.007,player6 reward: 0.017000000000000008,\n",
      "time step:151 player1 reward: 0.5000000000000003,player2 reward: 187.5,player3 reward: 145.611,player4 reward: 144.11599999999999,player5 reward: 177.06,player6 reward: 165.05,\n",
      "time step:161 player1 reward: 0.5000000000000003,player2 reward: 0.0,player3 reward: 0.5000000000000003,player4 reward: 0.05,player5 reward: 0.15,player6 reward: 0.5580000000000003,\n",
      "time step:171 player1 reward: 181.017,player2 reward: 99.236,player3 reward: 0.0,player4 reward: 89.93700000000003,player5 reward: 92.348,player6 reward: 0.2970000000000001,\n",
      "time step:181 player1 reward: 0.0,player2 reward: 187.001,player3 reward: 0.5000000000000003,player4 reward: 108.12000000000002,player5 reward: 29.705000000000005,player6 reward: 30.02,\n",
      "time step:191 player1 reward: 66.324,player2 reward: 187.5,player3 reward: 187.5,player4 reward: 67.72,player5 reward: 250.0,player6 reward: 0.3750000000000003,\n",
      "time step:201 player1 reward: 0.0,player2 reward: 187.5,player3 reward: 0.0,player4 reward: 0.67,player5 reward: 0.153,player6 reward: 0.263,\n",
      "time step:211 player1 reward: 0.5000000000000003,player2 reward: 0.5000000000000003,player3 reward: 0.5000000000000003,player4 reward: 0.0,player5 reward: -0.346,player6 reward: 20.160999999999994,\n",
      "time step:221 player1 reward: 0.5000000000000003,player2 reward: 187.5,player3 reward: 0.0,player4 reward: -0.07399999999999994,player5 reward: 138.05,player6 reward: 30.294999999999998,\n",
      "time step:231 player1 reward: 0.0,player2 reward: 0.0,player3 reward: 153.08999999999997,player4 reward: 0.0,player5 reward: 0.08800000000000006,player6 reward: 98.523,\n",
      "time step:241 player1 reward: 0.5000000000000003,player2 reward: 187.5,player3 reward: 0.0,player4 reward: 0.22099999999999997,player5 reward: 0.05,player6 reward: 0.15,\n",
      "time step:251 player1 reward: 187.5,player2 reward: 187.5,player3 reward: 0.0,player4 reward: 0.05,player5 reward: 0.15000000000000002,player6 reward: 0.05,\n",
      "time step:261 player1 reward: 2.9939999999999665,player2 reward: 0.5000000000000003,player3 reward: 0.0,player4 reward: 1.154999999999989,player5 reward: 0.5160000000000002,player6 reward: 0.176,\n",
      "time step:271 player1 reward: 0.5000000000000003,player2 reward: 136.6359999999999,player3 reward: 40.5,player4 reward: 0.171,player5 reward: 30.009999999999998,player6 reward: -0.47,\n",
      "time step:281 player1 reward: 0.5000000000000003,player2 reward: 187.5,player3 reward: 0.5000000000000003,player4 reward: 0.201,player5 reward: 0.21100000000000002,player6 reward: 187.5,\n",
      "time step:291 player1 reward: 187.5,player2 reward: 187.5,player3 reward: 0.5000000000000003,player4 reward: 187.5,player5 reward: 187.5,player6 reward: 187.5,\n",
      "time step:301 player1 reward: 0.5000000000000003,player2 reward: 0.5000000000000003,player3 reward: 156.5830000000004,player4 reward: 0.44100000000000017,player5 reward: 0.0,player6 reward: 3.75,\n",
      "time step:311 player1 reward: 0.5000000000000003,player2 reward: 0.5000000000000003,player3 reward: 0.5000000000000003,player4 reward: 0.48000000000000037,player5 reward: 0.5060000000000003,player6 reward: 0.0,\n",
      "time step:321 player1 reward: 187.5,player2 reward: 93.7500000000012,player3 reward: 187.0,player4 reward: 0.0,player5 reward: 56.5,player6 reward: 0.0,\n",
      "time step:331 player1 reward: 188.0,player2 reward: 0.0,player3 reward: 0.0,player4 reward: 0.010000000000000002,player5 reward: 250.0,player6 reward: 16.0,\n",
      "time step:341 player1 reward: 0.5000000000000003,player2 reward: 16.958000000000535,player3 reward: 0.0,player4 reward: 0.09300000000000007,player5 reward: 0.8690000000000007,player6 reward: 8.006999999999998,\n",
      "time step:351 player1 reward: 81.28500000000004,player2 reward: 186.5,player3 reward: 62.833999999999236,player4 reward: 144.5,player5 reward: 11.059000000000001,player6 reward: 84.50900000000003,\n",
      "time step:361 player1 reward: 0.5000000000000003,player2 reward: 187.5,player3 reward: 0.0,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:371 player1 reward: 0.0,player2 reward: 181.5,player3 reward: 187.5,player4 reward: 38.0,player5 reward: 83.0,player6 reward: 0.0,\n",
      "time step:381 player1 reward: 190.0,player2 reward: 0.0,player3 reward: 187.0,player4 reward: 6.0,player5 reward: 0.5,player6 reward: 36.5,\n",
      "time step:391 player1 reward: 0.5000000000000003,player2 reward: 0.5000000000000003,player3 reward: 0.0,player4 reward: 0.005,player5 reward: 0.007,player6 reward: 0.0,\n",
      "time step:401 player1 reward: 0.0,player2 reward: 0.5000000000000003,player3 reward: 187.5,player4 reward: 0.004,player5 reward: 0.004,player6 reward: 0.0,\n",
      "time step:411 player1 reward: 0.5000000000000003,player2 reward: 187.5,player3 reward: 0.5000000000000003,player4 reward: 186.0,player5 reward: 0.5000000000000003,player6 reward: 186.5,\n",
      "time step:421 player1 reward: 155.0,player2 reward: 165.558,player3 reward: 180.519,player4 reward: 2.0,player5 reward: 30.0,player6 reward: 0.0,\n",
      "time step:431 player1 reward: 0.0,player2 reward: 182.0,player3 reward: 183.012,player4 reward: 0.3760000000000003,player5 reward: 0.3750000000000003,player6 reward: 174.00900000000001,\n",
      "time step:441 player1 reward: 0.0,player2 reward: 0.5000000000000003,player3 reward: 187.5,player4 reward: 136.5,player5 reward: 63.0,player6 reward: 11.5,\n",
      "time step:451 player1 reward: 31.417000000000506,player2 reward: 0.0,player3 reward: 31.91600000000051,player4 reward: 1.5079999999999991,player5 reward: 0.017000000000000008,player6 reward: 0.006,\n",
      "time step:461 player1 reward: 0.5000000000000003,player2 reward: 0.5000000000000003,player3 reward: 0.5000000000000003,player4 reward: 0.006,player5 reward: 0.3730000000000003,player6 reward: 0.3750000000000003,\n",
      "time step:471 player1 reward: 187.5,player2 reward: 0.0,player3 reward: 187.5,player4 reward: 185.0,player5 reward: 0.0,player6 reward: 0.016000000000000007,\n",
      "time step:481 player1 reward: 3.491999999999946,player2 reward: 165.06,player3 reward: 0.5000000000000003,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 182.50799999999998,\n",
      "time step:491 player1 reward: 0.5000000000000003,player2 reward: 0.5000000000000003,player3 reward: 0.5000000000000003,player4 reward: 0.022000000000000013,player5 reward: 0.36500000000000027,player6 reward: 0.47200000000000036,\n",
      "time step:501 player1 reward: 186.0,player2 reward: 189.5,player3 reward: 185.5,player4 reward: 11.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:511 player1 reward: 40.891999999999996,player2 reward: 41.888999999999996,player3 reward: 0.5000000000000003,player4 reward: 0.004,player5 reward: 0.2990000000000002,player6 reward: 0.1520000000000001,\n",
      "time step:521 player1 reward: 187.5,player2 reward: 187.5,player3 reward: 187.5,player4 reward: 0.0,player5 reward: 11.0,player6 reward: 0.0,\n",
      "time step:531 player1 reward: 0.0,player2 reward: 0.5000000000000003,player3 reward: 186.503,player4 reward: 0.009000000000000001,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:541 player1 reward: 187.5,player2 reward: 187.5,player3 reward: 0.0,player4 reward: 171.5,player5 reward: 0.003,player6 reward: 101.5,\n",
      "time step:551 player1 reward: 0.5000000000000003,player2 reward: 0.0,player3 reward: 187.5,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:561 player1 reward: 0.5000000000000003,player2 reward: 0.0,player3 reward: 187.5,player4 reward: 186.5,player5 reward: 0.0,player6 reward: 0.3730000000000003,\n",
      "time step:571 player1 reward: 4.488000000000163,player2 reward: 0.5000000000000003,player3 reward: 1.9959999999999454,player4 reward: 0.04100000000000003,player5 reward: 0.08600000000000006,player6 reward: 0.024000000000000014,\n",
      "time step:581 player1 reward: 4.9870000000001555,player2 reward: 16.457999999999853,player3 reward: 0.5000000000000003,player4 reward: 0.0,player5 reward: 4.006000000000001,player6 reward: 0.0,\n",
      "time step:591 player1 reward: 183.51,player2 reward: 0.0,player3 reward: 0.5000000000000003,player4 reward: 0.5,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:601 player1 reward: 0.0,player2 reward: 0.5000000000000003,player3 reward: 169.548,player4 reward: 0.4960000000000004,player5 reward: 0.43400000000000033,player6 reward: 166.017,\n",
      "time step:611 player1 reward: 0.0,player2 reward: 0.5000000000000003,player3 reward: 186.005,player4 reward: 0.0,player5 reward: 0.09000000000000007,player6 reward: 0.003,\n",
      "time step:621 player1 reward: 187.5,player2 reward: 0.0,player3 reward: 0.0,player4 reward: 0.3750000000000003,player5 reward: 180.5,player6 reward: 0.3750000000000003,\n",
      "time step:631 player1 reward: 121.5,player2 reward: 0.5000000000000003,player3 reward: 187.5,player4 reward: 1.9709999999999481,player5 reward: 100.22899999999997,player6 reward: 0.0,\n",
      "time step:641 player1 reward: 187.5,player2 reward: 187.5,player3 reward: 0.5000000000000003,player4 reward: 0.06900000000000005,player5 reward: 0.01800000000000001,player6 reward: 0.0,\n",
      "time step:651 player1 reward: 0.0,player2 reward: 0.5000000000000003,player3 reward: 191.0,player4 reward: 233.5,player5 reward: 0.3790000000000003,player6 reward: 0.3740000000000003,\n",
      "time step:661 player1 reward: 0.5000000000000003,player2 reward: 0.0,player3 reward: 0.0,player4 reward: 0.4960000000000004,player5 reward: 0.001,player6 reward: 0.45600000000000035,\n",
      "time step:671 player1 reward: 187.5,player2 reward: 0.5000000000000003,player3 reward: 187.5,player4 reward: 0.3750000000000003,player5 reward: 0.3750000000000003,player6 reward: 245.0,\n",
      "time step:681 player1 reward: 0.5000000000000003,player2 reward: 187.5,player3 reward: 187.5,player4 reward: 186.0,player5 reward: 186.5,player6 reward: 186.5,\n",
      "time step:691 player1 reward: 187.5,player2 reward: 0.5000000000000003,player3 reward: 187.5,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:701 player1 reward: 0.5000000000000003,player2 reward: 186.5,player3 reward: 187.5,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 9.0,\n",
      "time step:711 player1 reward: 187.5,player2 reward: 187.5,player3 reward: 187.5,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:721 player1 reward: 0.5000000000000003,player2 reward: 0.0,player3 reward: 187.5,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:731 player1 reward: 0.5000000000000003,player2 reward: 0.21600000000000016,player3 reward: 187.5,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:741 player1 reward: 0.5000000000000003,player2 reward: 0.5000000000000003,player3 reward: 0.0,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:751 player1 reward: 0.0,player2 reward: 180.5,player3 reward: 0.0,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:761 player1 reward: 187.5,player2 reward: 187.5,player3 reward: 0.0,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:771 player1 reward: 187.5,player2 reward: 0.5000000000000003,player3 reward: 0.5000000000000003,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:781 player1 reward: 187.5,player2 reward: 0.5000000000000003,player3 reward: 0.5000000000000003,player4 reward: 0.0,player5 reward: 0.0,player6 reward: 0.0,\n",
      "time step:791 player1 reward: 187.5,player2 reward: 187.5,player3 reward: 0.5000000000000003,player4 reward: 186.0,player5 reward: 0.34800000000000025,player6 reward: 186.5,\n",
      "time step:801 player1 reward: 0.44300000000000034,player2 reward: 0.4990000000000004,player3 reward: 0.4950000000000004,player4 reward: 0.8540000000000006,player5 reward: 0.6310000000000004,player6 reward: 0.0,\n",
      "time step:811 player1 reward: 1.5,player2 reward: 174.001,player3 reward: 0.5000000000000003,player4 reward: 0.36900000000000027,player5 reward: 0.0,player6 reward: 164.0,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeStep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataNum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 126\u001b[0m, in \u001b[0;36mMyPPO.learn\u001b[1;34m(self, timeStep, dataNum, lr, episode, epoch, batchSize)\u001b[0m\n\u001b[0;32m    124\u001b[0m             playtime_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatchSize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\Desktop\\gameAI\\weiso_tank_env\\PPO.py:149\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self, batch_size, epochs, gamma, lr, episode, lmbda)\u001b[0m\n\u001b[0;32m    145\u001b[0m optimizer_value\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mdetect_anomaly():\n\u001b[1;32m--> 149\u001b[0m     \u001b[43mpolicy_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     value_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    152\u001b[0m replace_nan_with_zero(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPolicyNetwork)\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.learn(timeStep=10000, lr=0.003, dataNum=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "for i in range(10):\n",
    "    agent.show()\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
